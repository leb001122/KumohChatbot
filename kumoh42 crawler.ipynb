{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f627131",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = OrderedDict()\n",
    "options = webdriver.ChromeOptions()\n",
    "path = \"C:\\\\Users\\\\DeepLearning_4\\\\Desktop\\\\driver\\\\chromedriver.exe\"\n",
    "\n",
    "driver = webdriver.Chrome(path , options=options)\n",
    "driver.set_window_size(360, 740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9614f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link():\n",
    "    page = 1\n",
    "    links = []\n",
    "    while True :\n",
    "        \n",
    "        if page == 67:\n",
    "            break\n",
    "        driver.get('https://kumoh42.com/index.php?mid=park&page=' + str(page))\n",
    "        req = driver.page_source\n",
    "        soup = BeautifulSoup(req, 'html.parser')\n",
    "\n",
    "        for tr in soup.select(\"tbody > tr\") :\n",
    "            if not tr.find(class_ = re.compile(\"notice\")) : \n",
    "                link = tr.find(href=re.compile(\"document_srl\"))\n",
    "                links.append(link.attrs['href'])\n",
    "\n",
    "        for link in links:\n",
    "            print(link)\n",
    "            \n",
    "        page += 1\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f508d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = make_link()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for link in links :\n",
    "    driver.get(link)\n",
    "    \n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, 'html.parser')\n",
    "    \n",
    "    commentNum = soup.select_one('#comment > div.eq.flex.flex-wrap > div.ed.flex.flex-left.flex-wrap.margin-top-small.margin-bottom-small > h4').text\n",
    "    num = re.sub(\"개의 댓글\", \"\", commentNum)\n",
    "    if int(num)==0 :\n",
    "        continue\n",
    "    \n",
    "    title = soup.select_one(\"#main > div:nth-child(2) > div > div > div.col-md-9 > div > div.ed.article-wrapper.inner-container > div.ed > div.ed.article-head.margin-bottom-large > h2 > a\").text\n",
    "    \n",
    "    content = \"\"\n",
    "    for data in soup.find_all(class_ = re.compile(\"^document\")) : \n",
    "        content += data.text\n",
    "\n",
    "    comments= []\n",
    "    for data in soup.select(\"div.ed.comment-content > div > div:nth-child(2) > div > p\"): \n",
    "        comments.append({\n",
    "          \"comment\" : data.text   \n",
    "        })\n",
    "#     print(comments)\n",
    "    \n",
    "#     date = soup.select_one(\"#main > div:nth-child(2) > div > div > div.col-md-9 > div > div.ed.article-wrapper.inner-container > div.ed > div.ed.article-head.margin-bottom-large > div > div.ed.flex.flex-wrap > span.ed.text-muted.margin-right-small\").text       \n",
    "#     # ~시간 전 | ~일 전 이라고 표시되는 게시물 존재함  \n",
    "#     if '전' in date:\n",
    "#         now = datetime.now()\n",
    "#         if '시간' in date:  # '~시간 전'\n",
    "#             date = str(now.year) + '.' + str(now.month) + '.' + str(now.day)\n",
    "#         else : # '~일 전'\n",
    "#             date = re.sub(\"일 전\", \"\", date)\n",
    "    json_dic = dict()\n",
    "    json_dic['title'] = title\n",
    "    json_dic['content'] = content\n",
    "    json_obj = dict()\n",
    "    json_obj = comments\n",
    "    json_dic['comments'] = json_obj\n",
    "    \n",
    "    output.append(json_dic)\n",
    "#     print(json_dic)\n",
    "\n",
    "#     data.append({\n",
    "# #         \"date\" : date,\n",
    "#         \"title\" : title,\n",
    "#         \"content\" : content,\n",
    "#         \"comments\" : comments\n",
    "# #         \"link\" : link\n",
    "#     }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"금오사이.json\", \"w\", encoding='UTF-8') as json_file:\n",
    "    json.dump(output, json_file, indent=4, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sig21",
   "language": "python",
   "name": "sig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
