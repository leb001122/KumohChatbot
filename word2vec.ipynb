{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f130a3",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f88885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessing(text):\n",
    "    text = okt.nouns(text)\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"kumohQnA.json\")\n",
    "\n",
    "stop_words = \"좀 가 을 를 이 께서 에서 에 에게 께 한테 더러 에서 에게서 한테서 에 에서 로 으로 으로서 의 과 와 하고 보다 처럼 만큼 아 야 이여 여 만 도 은 는 조차 마저 까지 부터 이나 나 이나마 나마 이라도 라도 이야 야 이라야 라야 대해 하나 거나 위해 안녕하세요 건가 는걸 일이 가지 무엇 이면 안녕하십니까 안해 위해 던데 내야 등등 수가 또한 거나 이건 그거 거기 그것 려고 어케 안나 대로 여러분 치면 처럼 그게 어찌 저런 된거 푸나 끼리 고해 다가 로만 일도 이란 따라서 이기 이나 님들 여야 번은 이구 라서 정이 번만 런가 하니 워낙 아마 안이 부터\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['question'].apply(textPreprocessing)\n",
    "\n",
    "print('전체 문서의 수 :',len(df))\n",
    "df['cleaned'].replace('', np.nan, inplace=True)\n",
    "df = df[df['cleaned'].notna()]\n",
    "print('전체 문서의 수 :',len(df))\n",
    "\n",
    "corpus = []\n",
    "for words in df['cleaned']:\n",
    "    corpus.append(words.split())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3672713",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6254d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('C:\\ko\\ko.bin')\n",
    "model.wv.save_word2vec_format(\"ko.bin.gz\", binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16696e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(size = 200, window=5, min_count = 3, workers = 4, sg=1)\n",
    "word2vec_model.build_vocab(corpus)\n",
    "word2vec_model.intersect_word2vec_format(\"ko.bin.gz\", lockf=1.0, binary=False)\n",
    "word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a875f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('동아리')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45e9af",
   "metadata": {},
   "source": [
    "## 문장 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c866b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors(document_list) :\n",
    "    document_embedding_list = []\n",
    "    \n",
    "    for text in document_list:\n",
    "        count = 0\n",
    "        doc2vec = None\n",
    "        for word in text.split():\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                count += 1\n",
    "                # 해단 문서에 있는 모든 단어들의 벡터값을 더한다.\n",
    "                if doc2vec is None : \n",
    "                    doc2vec = word2vec_model.wv[word]\n",
    "                else : \n",
    "                    doc2vec = doc2vec + word2vec_model.wv[word]\n",
    "                \n",
    "        if doc2vec is not None :\n",
    "            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\n",
    "            doc2vec = doc2vec/count\n",
    "            document_embedding_list.append(doc2vec)\n",
    "            \n",
    "    return document_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199c4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_embedding_list = vectors(df['cleaned'])\n",
    "print('문서 벡터의 수 :', len(document_embedding_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88897176",
   "metadata": {},
   "source": [
    "## 사용자 입력 전처리 및 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectors(text) :\n",
    "    text_embedding_list = []\n",
    "    \n",
    "    count = 0\n",
    "    doc2vec = None\n",
    "    for word in text.split():\n",
    "        if word in word2vec_model.wv.vocab:\n",
    "            count += 1\n",
    "            # 해단 문서에 있는 모든 단어들의 벡터값을 더한다.\n",
    "            if doc2vec is None : \n",
    "                doc2vec = word2vec_model.wv[word]\n",
    "            else : \n",
    "                doc2vec = doc2vec + word2vec_model.wv[word]\n",
    "\n",
    "    if doc2vec is not None :\n",
    "        # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\n",
    "        doc2vec = doc2vec/count\n",
    "        text_embedding_list.append(doc2vec)\n",
    "            \n",
    "    return text_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33875bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_input = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eec2a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = textPreprocessing(user_input)\n",
    "print(text)\n",
    "text_embedding = text_vectors(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ac589",
   "metadata": {},
   "source": [
    "## 코사인 유사도 매트릭스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec85a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = cosine_similarity(document_embedding_list, text_embedding)\n",
    "print('코사인 유사도 매트릭스의 크기 :',cosine_similarities.shape)\n",
    "\n",
    "sim_scores = list(enumerate(cosine_similarities))\n",
    "sim_scores = sorted(sim_scores, key=lambda x : x[1], reverse=True)\n",
    "sim_scores = sim_scores[:5]\n",
    "\n",
    "indices= [i[0] for i in sim_scores] # 가장 유사한 질문 5개의 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3f05",
   "metadata": {},
   "source": [
    "## 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4830d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QnA = df[['question', 'answer']]\n",
    "result = QnA.iloc[indices]\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sig21",
   "language": "python",
   "name": "sig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
